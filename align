#!/usr/bin/env python
import optparse, pdb, math, operator
from decimal import Decimal
import sys
from collections import defaultdict


def sorte(t):
  return sorted(t.items(), key=operator.itemgetter(1))

def generate_bitext(f_data, e_data, num_sents):
    return [[sentence.strip().split() for sentence in pair]
        for pair in zip(open(f_data), open(e_data))[:num_sents]]

def train(f_data, e_data, num_sents):
    IGNORED = '1 2 3 4 5 6 7 8 9 0 $ ( ) [ ] ? ! . - ,'.split(' ')
 
    sys.stderr.write("Training via IBM 1...")
    bitext = generate_bitext(f_data, e_data, num_sents)

    converge = False

    f_count = defaultdict(int)
    t = defaultdict(float)
    #for (n, (f,e)) in enumerate(bitext):
    #    for f_i in set(f):
    #        f_count[f_i] = 0

    '''for (n, (f,e)) in enumerate(bitext):
        for f_i in set(f):
            if f_i not in IGNORED:
                for e_j in set(e):
                    if e_j not in IGNORED:
                        t[(f_i, e_j)] = 1.0 / len(e)
    '''


    fe_count = defaultdict(int)
    s = defaultdict(float)
    tot_fe_count = 0.0
    tot_f = defaultdict(float)
    perf_match = defaultdict(str)

    for (n, (f, e)) in enumerate(bitext):
        #sys.stderr.write(str(n) + '\n')
        curr_fes = []
        e_count = defaultdict(int)
        for f_i in set(f):
            tot_fe_count = 0.0

            #t[f_i] = 0
            f_count[f_i] += 1
            for e_j in set(e):
                fe_count[(f_i, e_j)] += 1
                t[(f_i, e_j)] = 1.0 / len(e)
                tot_fe_count += 1
                s[(f_i, e_j)] = 0
                curr_fes.append((f_i, e_j))
        for e_j in set(e):
            if e_j not in e_count:
                e_count[e_j] = 0.0
            e_count[e_j] += 1
        if n % 100 == 0:
            sys.stderr.write(".")

        #Normalization
        for tup in curr_fes:
            #fe_count[tup] = fe_count[tup] / e_count[tup[1]]
            s[tup] += fe_count[tup]
    x = 0

    while x < 50:

        sys.stderr.write(str(x))

        for (n, (f, e)) in enumerate(bitext):
            #Collecting counts
            e = set(e)
            e.difference_update(IGNORED)
            f = set(f)
            f.difference_update(IGNORED) 
            for e_j in e:
                for f_i in f:
                    #pdb.set_trace()
                    fe_count[(f_i, e_j)] += t[(f_i, e_j)] / (s[(f_i, e_j)])
                    tot_f[f_i] += t[(f_i, e_j)] / (s[(f_i, e_j)])

            #estimate probabilities
            for f_i in f:
                for e_j in e:
                    t[(f_i, e_j)] = fe_count[(f_i, e_j)] / tot_f[f_i]
                    #if t[(f_i,e_j)] == 1:
                        #perf_match[f_i] = e_j
                        #del t[(f_i,e_j)]

        #if max(t.iteritems(), key=operator.itemgetter(1))[1] >= opts.threshold:
            #converge = True
        x += 1

    pdb.set_trace()
    return t

def merge_maps(t1, t2):
    matches = {}
    t1 = sorte(t1)
    t2 = sorte(t2)
    while t1:
        curr = t1[-1]
        backwards = reversed(curr[0])
        if backwards in t2:
            prob_sum = curr[1] + t2[[i[0] for i in t2].index(backwards)][1] #gives us the backwards probability
            if prob_sum > 1.5:
                matches[curr[0]] = curr[1] 
        t1.pop()

    return matches

def generate_a(f_data, e_data, t, num_sents, threshold=.5,):
    bitext = generate_bitext(f_data, e_data, num_sents)

    for (f, e) in bitext:
        for (i, f_i) in enumerate(f):
            for (j, e_j) in enumerate(e):
                if t[(f_i, e_j)] >= threshold:
                    sys.stdout.write("%i-%i " % (i, j))
        sys.stdout.write("\n")


def main():

    optparser = optparse.OptionParser()
    optparser.add_option("-d", "--data",
                         dest="train",
                         default="data/hansards",
                         help="Data filename prefix (default=data)")
    optparser.add_option("-e", "--english",
                         dest="english",
                         default="e",
                         help="Suffix of English filename (default=e)")
    optparser.add_option("-f", "--french",
                         dest="french",
                         default="f",
                         help="Suffix of French filename (default=f)")
    optparser.add_option("-t", "--threshold",
                         dest="threshold",
                         default=0.5, type="float",
                         help="Threshold for aligning (default=0.5)")
    optparser.add_option("-n", "--num_sentences",
                         dest="num_sents",
                         default=sys.maxint, type="int",
                         help="Number of sentences to train and align")

    (opts, _) = optparser.parse_args()
    f_data = "%s.%s" % (opts.train, opts.french)
    e_data = "%s.%s" % (opts.train, opts.english)

    f_e = train(f_data, e_data, opts.num_sents)
    #e_f = train(e_data, f_data, opts.num_sents)

    generate_a(f_data, e_data, f_e, opts.num_sents, opts.threshold)#merge_maps(f_e, e_f), opts.threshold)


if __name__ == '__main__':
    main()




'''
dice = defaultdict(int)
for (k, (f_i, e_j)) in enumerate(fe_count.keys()):
    dice[(f_i, e_j)] = 2.0 * fe_count[(f_i, e_j)] / (f_count[f_i] + e_count[e_j])
    if k % 5000 == 0:
        sys.stderr.write(".")
sys.stderr.write("\n")
'''
